# In this example we run Data Parallelism with 2 nodes (4 GPUs each node) using the ViT-H model from Google ViTs. 

# Uncomment the following line if you want experiments to run one after the another
# sequential: true

variables:
  # System info (comment as needed)
  system: ['alps']
  partition: ['normal']
  comm_lib: ['nccl']

  # system: ['leonardo']
  # partition: ['boost_usr_prod-1']
  # comm_lib: ['nccl']

  # ------------------------------------------

  # The number of nodes can be incremented to have a better chance to satisfy the placement 
  nodes: [2]
  dp_model: ['vit_h_16'] # 16 indicate the local batch size, you can change to 32, 64 or 128
  fsdp_model: ['llama3_8b_16']
  hybrid2d_model: ['minerva_7b_16']
  hybrid3d_model: ['llama3_70b_16']
  hybrid3d_moe_model: ['mixtral_8x7b_16']

# In case it is not possible to satisfy placements constraints, this will fail with return code 456
preprocess: |
  echo "Allocated nodes: $SLURM_JOB_NODELIST"
  echo "SKIPPING allocation check..."

jobs:
  # The number of nodes can be incremented to have a better chance to satisfy the placement 
  - config: "{system}__{partition}__{nodes}_nodes"
    config_jobs:

      - tag: "dp_{dp_model}_{comm_lib}_distance_unknown"
        variables:
          num_buckets: [50]
        command: "srun -N {nodes} ./DLNetBench/bin/{comm_lib}/cpp/data_parallel/dp {dp_model} {num_buckets} ./DLNetBench"
      
      - tag: "fsdp_{fsdp_model}_{comm_lib}_distance_unknown"
        variables:
          num_units: [16]
          sharding_factor: [4]
        command: "srun -N {nodes} ./DLNetBench/bin/{comm_lib}/cpp/data_parallel/fsdp {fsdp_model} {num_units} {sharding_factor} ./DLNetBench"
      
      # - tag: "hybrid2d_{hybrid2d_model}_{comm_lib}_distance_unknown"
      #   variables: 
      #     num_stages: [4]
      #     num_microbatches: [16]
      #   command: "srun -N {nodes} ./DLNetBench/bin/{comm_lib}/cpp/hybrid_parallel/hybrid_2d {hybrid2d_model} {num_stages} {num_microbatches} ./DLNetBench"
               